{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped IDs from page 21\n",
      "Scraped IDs from page 22\n",
      "Scraped IDs from page 23\n",
      "Scraped IDs from page 24\n",
      "Scraped IDs from page 25\n",
      "Scraped IDs from page 26\n",
      "Scraped IDs from page 27\n",
      "Scraped IDs from page 28\n",
      "Scraped IDs from page 29\n",
      "Scraped IDs from page 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m num_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Get data crawled\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m product_ids, data_product_ids, links, title, price, discount, review_count, quantity_sold, product_variant \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_all_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Create dataframe for data crawled\u001b[39;00m\n\u001b[0;32m    100\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(product_ids, data_product_ids, links, title,\n\u001b[0;32m    102\u001b[0m         price, discount, quantity_sold, product_variant, review_count\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     ]\n\u001b[0;32m    108\u001b[0m )\n",
      "Cell \u001b[1;32mIn[1], line 72\u001b[0m, in \u001b[0;36mcrawl_all_ids\u001b[1;34m(driver, start_url, num_pages)\u001b[0m\n\u001b[0;32m     69\u001b[0m     sleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraped IDs from page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m product_ids, data_product_ids, links, title, price, discount, review_count, quantity_sold, product_variant\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import libraby\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"D:/HOC KI 8/3. Graduate project/hasaki_crawling\"\n",
    "\n",
    "# Get review counts:\n",
    "def get_reviews_counts(string):\n",
    "    start_index = string.find('(')\n",
    "    end_index = string.find(')')\n",
    "    review_count = string[start_index + 1:end_index]\n",
    "    return review_count\n",
    "\n",
    "# Create funtion to crawl\n",
    "def crawl_all_ids(driver, start_url, num_pages):\n",
    "\n",
    "    # GET INFOMATION OF ALL ITEMS\n",
    "    product_ids, data_product_ids, links, title = [], [], [], []\n",
    "    price, discount, review_count, quantity_sold, product_variant = [], [], [], [], []\n",
    "\n",
    "    for page in range(1, 20 + 1):\n",
    "\n",
    "        # Get url\n",
    "        url = f\"{start_url}?p={page}\"\n",
    "        driver.get(url)\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get product-id, data-product-id, link\n",
    "        elems_link_id = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_info_item_sp')\n",
    "        product_ids = [elem.get_attribute('data-id') for elem in elems_link_id] + product_ids\n",
    "        data_product_ids = [elem.get_attribute('data-product') for elem in elems_link_id] + data_product_ids\n",
    "        links = [elem.get_attribute('href') for elem in elems_link_id] + links\n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        # Get title\n",
    "        elems_title = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .vn_names')\n",
    "        title = [elem.text for elem in elems_title] + title\n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        # Get price\n",
    "        elems_price = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giacu')\n",
    "        price = [elem.text for elem in elems_price] + price  \n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        # Get discount\n",
    "        elems_discount = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giamoi')\n",
    "        discount = [elem.text for elem in elems_discount] + discount\n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        # Get review counts:\n",
    "        elems_review_count = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_count_by')\n",
    "        review_count = [get_reviews_counts(elem.text) for elem  in elems_review_count] + review_count\n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        # Get quantity_sold\n",
    "        elems_quantity_sold = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_count_by')\n",
    "        quantity_sold = [elem.text.strip('\\n ') for elem  in elems_quantity_sold] + quantity_sold\n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        # Get product_variant\n",
    "        elems_product_variant = driver.find_elements(By.CSS_SELECTOR, \".ProductGrid__grid .block_info_item_sp\")\n",
    "        product_variant = [elem.get_attribute('data-variant') for elem  in elems_product_variant] + product_variant\n",
    "        sleep(random.randint(2,5))\n",
    "\n",
    "        print(f\"Scraped IDs from page {page}\")\n",
    "        sleep(random.randint(15,20))\n",
    "    \n",
    "    return product_ids, data_product_ids, links, title, price, discount, review_count, quantity_sold, product_variant\n",
    "\n",
    "# Add user agent\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Setting Options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize browsser\n",
    "driver = webdriver.Chrome(options= options)\n",
    "\n",
    "# URL to crawl\n",
    "start_url = \"https://hasaki.vn/danh-muc/cham-soc-co-the-c12.html?\"\n",
    "\n",
    "# Number pages to crawl\n",
    "num_pages = 20\n",
    "\n",
    "# Get data crawled\n",
    "product_ids, data_product_ids, links, title, price, discount, review_count, quantity_sold, product_variant = crawl_all_ids(driver, start_url, num_pages)\n",
    "\n",
    "# Create dataframe for data crawled\n",
    "data = pd.DataFrame(\n",
    "    list(zip(product_ids, data_product_ids, links, title,\n",
    "        price, discount, quantity_sold, product_variant, review_count\n",
    "    )),\n",
    "    columns=[\n",
    "        'product_id', 'data_product_id', 'link_item', 'title', \n",
    "        'original_price', 'current_price', 'quantity_sold', 'product_variant','review_count'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save into csv\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "productdata_filename = f\"productdata_{current_datetime}.csv\"\n",
    "\n",
    "data.to_csv(os.path.join(folder_path, \"data\", productdata_filename), encoding= \"utf-8-sig\")\n",
    "print(\"All product IDs saved to CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
