{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraby\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"D:/HOC KI 8/3. Graduate project/hasaki_crawling/data\"\n",
    "\n",
    "# Add user agent\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Setting Options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Get review counts:\n",
    "def get_reviews_counts(string):\n",
    "    start_index = string.find('(')\n",
    "    end_index = string.find(')')\n",
    "    review_count = string[start_index + 1:end_index]\n",
    "    return review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19325', '65994', '12684', '86167', '34119', '119084', '99221', '9740', '102959', '11089', '117680', '96523', '43795', '19286', '106681', '106533', '2364', '103543', '89563', '90023', '87859', '105523', '90339', '12064', '102911', '91747', '81465', '90639', '68810', '90401', '66026', '102957', '106679', '43789', '102593', '102185', '3495', '71263', '4359', '76584']\n",
      "Scraped IDs from page 1\n",
      "['96589', '93797', '102963', '86161', '100457', '74010', '75956', '7947', '99715', '99707', '94529', '114501', '845', '96659', '117684', '56056', '84667', '98921', '1746', '1278', '95523', '83415', '66054', '95711', '102853', '92917', '96677', '100971', '6258', '79881', '92215', '1742', '102343', '114435', '98637', '101791', '61216', '104259', '89019', '100459']\n",
      "Scraped IDs from page 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize browsser\n",
    "driver = webdriver.Chrome(options= options)\n",
    "\n",
    "# URL to crawl\n",
    "start_url = \"https://hasaki.vn/danh-muc/cham-soc-da-mat-c4.html\"\n",
    "\n",
    "# GET INFOMATION OF ALL ITEMS\n",
    "# (1, 20+1) ~~ (1-20)\n",
    "# (21, 40+1) ~~ (21-40)\n",
    "# (1, 30+1) ~~ (1-30)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for page in range(1, 3):\n",
    "    \n",
    "    product_ids, data_product_ids, links, title = [], [], [], []\n",
    "    price, discount, review_count, quantity_sold, product_variant = [], [], [], [], []\n",
    "\n",
    "    # Get url\n",
    "    url = f\"{start_url}?p={page}\"\n",
    "    driver.get(url)\n",
    "    sleep(random.randint(7,8))\n",
    "\n",
    "    # Get product-id, data-product-id, link\n",
    "    elems_link_id = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_info_item_sp')\n",
    "    product_ids = [elem.get_attribute('data-id') for elem in elems_link_id] + product_ids\n",
    "    data_product_ids = [elem.get_attribute('data-product') for elem in elems_link_id] + data_product_ids\n",
    "    links = [elem.get_attribute('href') for elem in elems_link_id] + links\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get title\n",
    "    elems_title = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .vn_names')\n",
    "    title = [elem.text for elem in elems_title] + title\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get price\n",
    "    elems_price = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giacu')\n",
    "    price = [elem.text for elem in elems_price] + price  \n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get discount\n",
    "    elems_discount = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giamoi')\n",
    "    discount = [elem.text for elem in elems_discount] + discount\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get review counts:\n",
    "    elems_review_count = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_count_by')\n",
    "    review_count = [get_reviews_counts(elem.text) for elem  in elems_review_count] + review_count\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get quantity_sold\n",
    "    elems_quantity_sold = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_count_by')\n",
    "    quantity_sold = [elem.text.strip('\\n ') for elem  in elems_quantity_sold] + quantity_sold\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get product_variant\n",
    "    elems_product_variant = driver.find_elements(By.CSS_SELECTOR, \".ProductGrid__grid .block_info_item_sp\")\n",
    "    product_variant = [elem.get_attribute('data-variant') for elem  in elems_product_variant] + product_variant\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    print(f\"Scraped IDs from page {page}\")\n",
    "    sleep(random.randint(7,8))\n",
    "\n",
    "    # Create dataframe for data crawled\n",
    "    product_data = pd.DataFrame(\n",
    "        list(zip(product_ids, data_product_ids, links, title,\n",
    "            price, discount, quantity_sold, product_variant, review_count\n",
    "        )),\n",
    "        columns=[\n",
    "            'product_id', 'data_product_id', 'link_item', 'title', \n",
    "            'original_price', 'current_price', 'quantity_sold', 'product_variant','review_count'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_list.append(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all comment crawled\n",
    "combined_product_data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All product IDs saved to CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Save into csv\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "productdata_filename = f\"productdata_{current_datetime}.csv\"\n",
    "\n",
    "combined_product_data.to_csv(os.path.join(folder_path, \"product\", productdata_filename), encoding= \"utf-8-sig\")\n",
    "print(\"All product IDs saved to CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
