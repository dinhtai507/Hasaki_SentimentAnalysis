{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from deep_translator import GoogleTranslator\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from underthesea import text_normalize\n",
    "from underthesea import classify\n",
    "from underthesea import sentiment\n",
    "import re\n",
    "\n",
    "# Read file\n",
    "mergecomment = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), \"data\", \"merged\", \"mergedcomment_20240402_1111.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link_item', 'data_product_id_list', 'data_product_id', 'name_comment',\n",
       "       'content_comment', 'product_variant', 'datetime_comment', 'rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list columns\n",
    "mergecomment.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 'content_comment' column\n",
    "mergecomment = mergecomment[['data_product_id', 'content_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_product_id</th>\n",
       "      <th>content_comment</th>\n",
       "      <th>lower_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65994</td>\n",
       "      <td>Giao lâu, chất lượng + đóng gói oke</td>\n",
       "      <td>giao lâu, chất lượng + đóng gói oke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65994</td>\n",
       "      <td>thích em này thật sự, mùi thơm thảo mộc, lành ...</td>\n",
       "      <td>thích em này thật sự, mùi thơm thảo mộc, lành ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65994</td>\n",
       "      <td>Toner này gần như ko mùi Dạng sệt thoa lên da ...</td>\n",
       "      <td>toner này gần như ko mùi dạng sệt thoa lên da ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65994</td>\n",
       "      <td>Vô thưởng vô phạt, dễ chịu, không mùi Thiết kế...</td>\n",
       "      <td>vô thưởng vô phạt, dễ chịu, không mùi thiết kế...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65994</td>\n",
       "      <td>giao hàng nhanh với cả xài rất tốt í</td>\n",
       "      <td>giao hàng nhanh với cả xài rất tốt í</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65994</td>\n",
       "      <td>Rất hài lòng</td>\n",
       "      <td>rất hài lòng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65994</td>\n",
       "      <td>toner dưỡng ẩm tốt, mùi thơm dễ chịu</td>\n",
       "      <td>toner dưỡng ẩm tốt, mùi thơm dễ chịu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65994</td>\n",
       "      <td>Sp tốt. Đóng gói cẩn thận. giao hàng trễ</td>\n",
       "      <td>sp tốt. đóng gói cẩn thận. giao hàng trễ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65994</td>\n",
       "      <td>Thực sự là mình ko thích loại này lắm, da mình...</td>\n",
       "      <td>thực sự là mình ko thích loại này lắm, da mình...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65994</td>\n",
       "      <td>da dầu như mình xài êm.thành phần dịu nhẹ</td>\n",
       "      <td>da dầu như mình xài êm.thành phần dịu nhẹ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_product_id                                    content_comment  \\\n",
       "0            65994                Giao lâu, chất lượng + đóng gói oke   \n",
       "1            65994  thích em này thật sự, mùi thơm thảo mộc, lành ...   \n",
       "2            65994  Toner này gần như ko mùi Dạng sệt thoa lên da ...   \n",
       "3            65994  Vô thưởng vô phạt, dễ chịu, không mùi Thiết kế...   \n",
       "4            65994               giao hàng nhanh với cả xài rất tốt í   \n",
       "5            65994                                       Rất hài lòng   \n",
       "6            65994               toner dưỡng ẩm tốt, mùi thơm dễ chịu   \n",
       "7            65994           Sp tốt. Đóng gói cẩn thận. giao hàng trễ   \n",
       "8            65994  Thực sự là mình ko thích loại này lắm, da mình...   \n",
       "9            65994          da dầu như mình xài êm.thành phần dịu nhẹ   \n",
       "\n",
       "                                       lower_comment  \n",
       "0                giao lâu, chất lượng + đóng gói oke  \n",
       "1  thích em này thật sự, mùi thơm thảo mộc, lành ...  \n",
       "2  toner này gần như ko mùi dạng sệt thoa lên da ...  \n",
       "3  vô thưởng vô phạt, dễ chịu, không mùi thiết kế...  \n",
       "4               giao hàng nhanh với cả xài rất tốt í  \n",
       "5                                       rất hài lòng  \n",
       "6               toner dưỡng ẩm tốt, mùi thơm dễ chịu  \n",
       "7           sp tốt. đóng gói cẩn thận. giao hàng trễ  \n",
       "8  thực sự là mình ko thích loại này lắm, da mình...  \n",
       "9          da dầu như mình xài êm.thành phần dịu nhẹ  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert to lower case\n",
    "mergecomment[\"lower_comment\"] = mergecomment[\"content_comment\"].str.lower()\n",
    "mergecomment.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Replace word\n",
    "\n",
    "# Read file abbreviations\n",
    "abbs_df = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), \"data\", \"abbreviations.csv\"))\n",
    "\n",
    "# Create a dictionary to map abbreviations to meanings\n",
    "abbreviation_dict = dict(zip(abbs_df['abbreviation'], abbs_df['meaning']))\n",
    "\n",
    "# Function to decode abbs\n",
    "def decode_abbreviations(text, abbreviation_dict):\n",
    "    # Search for abbreviations in the text and replace them \n",
    "    # with their corresponding meanings from the dictionary\n",
    "    for abbreviation, meaning in abbreviation_dict.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbreviation) + r'\\b', meaning, text)\n",
    "    return text\n",
    "\n",
    "# Apply the decode_abbreviations function to the 'content_comment' column\n",
    "mergecomment[\"decoded_comment\"] = mergecomment[\"lower_comment\"].apply(lambda x: decode_abbreviations(x, abbreviation_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Translate to English\n",
    "# Function to translate a batch of sentences from Vietnamese to English\n",
    "def translate_batch_vietnamese_to_english(texts):\n",
    "    translated_texts = GoogleTranslator(source='vi', target='en').translate_batch(texts)\n",
    "    return translated_texts\n",
    "\n",
    "# Specify the range of rows you want to translate\n",
    "# 0-500\n",
    "# 500-1000\n",
    "# 1000-1500\n",
    "# 1500-2000\n",
    "# 2000-2500\n",
    "# 2500-3000\n",
    "# 3000-3500\n",
    "# 3500-4000\n",
    "# 4000-4500\n",
    "# 4500-5000\n",
    "# 5000-end\n",
    "# start_index = 5000\n",
    "# end_index = len(mergecomment)\n",
    "\n",
    "# # Assign the translated comments to the DataFrame directly\n",
    "# mergecomment.loc[start_index:end_index-1, 'translated_comment'] = translate_batch_vietnamese_to_english(mergecomment['decoded_comment'][start_index:end_index].tolist())\n",
    "\n",
    "# # Save into csv\n",
    "# current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "# preprocessed_comment_filename = f\"preprocessed_comment_{current_datetime}.csv\"\n",
    "\n",
    "# mergecomment.to_csv(os.path.join(os.path.dirname(os.getcwd()), \"data\", \"preprocessed\", \"preprocessed_comment\", preprocessed_comment_filename), encoding= \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert emoji to text\n",
    "translated_comment = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), \"data\", \"preprocessed\", \"preprocessed_comment\", \"preprocessed_comment_20240403_1507.csv\"))\n",
    "\n",
    "# Function converting emoji to text\n",
    "def demojize_if_str(text):\n",
    "    if isinstance(text, str): \n",
    "        return emoji.demojize(text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Remove rows with NaN values in the \"translated_comment\" column\n",
    "translated_comment.dropna(subset=[\"translated_comment\"], inplace=True)\n",
    "\n",
    "# Apply the demojize function to the \"translated_comment\" column\n",
    "translated_comment[\"demojized_comment\"] = translated_comment[\"translated_comment\"].str.lower().apply(demojize_if_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove special characters\n",
    "translated_comment['standardlized_comment'] = translated_comment['demojized_comment'].apply(lambda text: re.sub(r'[^a-zA-Z0-9\\s]', '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Tokenize, Remove stopwords, Lemmatize\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define custom stop words related to cosmetics\n",
    "custom_stopwords = [\n",
    "    \"makeup\", \"skincare\", \"beauty\", \"cosmetics\", \"lotion\", \"serum\", \"moisturizer\", \"foundation\", \"lipstick\", \"mascara\",\n",
    "    \"eyeliner\", \"blush\", \"concealer\", \"highlighter\", \"eyeshadow\", \"lip\", \"face\", \"eyes\", \"skin\", \"hair\", \"brush\", \"powder\",\n",
    "    \"cream\", \"gel\", \"toner\", \"cleanser\", \"exfoliator\", \"primer\", \"sunscreen\", \"toning\", \"cleansing\", \"exfoliating\", \"hydrating\",\n",
    "    \"soothing\", \"brightening\", \"anti-aging\", \"wrinkle\", \"acne\", \"pore\", \"oily\", \"dry\", \"sensitive\", \"combination\", \"matte\",\n",
    "    \"glossy\", \"shimmer\", \"natural\", \"organic\", \"vegan\", \"cruelty-free\", \"fragrance\", \"scent\", \"perfume\", \"cologne\", \"aroma\",\n",
    "    \"essence\", \"floral\", \"fruity\", \"woody\", \"musk\", \"vanilla\", \"jasmine\", \"rose\", \"lavender\", \"citrus\", \"patchouli\",\n",
    "    \"sandalwood\", \"bergamot\", \"amber\", \"oud\", \"aquatic\", \"oriental\", \"gourmand\", \"spicy\", \"fresh\", \"clean\", \"sweet\",\n",
    "    \"floral\", \"woody\", \"fruity\", \"citrusy\", \"powdery\", \"green\", \"herbal\", \"aromatic\", \"musky\", \"sensual\", \"romantic\",\n",
    "    \"exotic\", \"elegant\", \"modern\", \"classic\", \"sophisticated\", \"feminine\", \"masculine\", \"unisex\", \"alluring\", \"captivating\",\n",
    "    \"enchanting\", \"intoxicating\"\n",
    "]\n",
    "\n",
    "# Combine the default stop words and custom stop words\n",
    "stop_words_combined = stop_words_spacy.union(custom_stopwords)\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(tokens):\n",
    "    # Lemmatize each token and join them back into a string\n",
    "    lemmatized_tokens = [token.lemma_ for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Function to tokenize, remove stop words, and lemmatize\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nlp(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token.text.lower() not in stop_words_combined]\n",
    "    # Lemmatize the filtered tokens\n",
    "    lemmatized_text = lemmatize_text(filtered_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'standardlized_comment' column and create a new column 'cleaned_comment'\n",
    "translated_comment['cleaned_comment'] = translated_comment['standardlized_comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "translated_comment_filename = f\"cleaned_comment_{current_datetime}.csv\"\n",
    "\n",
    "translated_comment.to_csv(os.path.join(os.path.dirname(os.getcwd()), \"data\", \"preprocessed\", \"preprocessed_comment\", translated_comment_filename), encoding= \"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
