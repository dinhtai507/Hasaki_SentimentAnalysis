{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped IDs from page 1\n",
      "Scraped IDs from page 2\n",
      "All product IDs saved to CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Import libraby\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Get review counts:\n",
    "def get_reviews_counts(string):\n",
    "    start_index = string.find('(')\n",
    "    end_index = string.find(')')\n",
    "    review_count = string[start_index + 1:end_index]\n",
    "    return review_count\n",
    "\n",
    "# Create funtion to crawl\n",
    "def crawl_all_ids(driver, start_url, num_pages):\n",
    "\n",
    "    # GET INFOMATION OF ALL ITEMS\n",
    "    product_ids, data_product_ids, links, title = [], [], [], []\n",
    "    price, discount, review_count, quantity_sold, product_variant = [], [], [], [], []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "\n",
    "        # Get url\n",
    "        url = f\"{start_url}?p={page}\"\n",
    "        driver.get(url)\n",
    "        sleep(random.randint(5,7))\n",
    "        \n",
    "        # Get product-id, data-product-id, link\n",
    "        elems_link_id = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_info_item_sp')\n",
    "        product_ids = [elem.get_attribute('data-id') for elem in elems_link_id] + product_ids\n",
    "        data_product_ids = [elem.get_attribute('data-product') for elem in elems_link_id] + data_product_ids\n",
    "        links = [elem.get_attribute('href') for elem in elems_link_id] + links\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get title\n",
    "        elems_title = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .vn_names')\n",
    "        title = [elem.text for elem in elems_title] + title\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get price\n",
    "        elems_price = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giacu')\n",
    "        price = [elem.text for elem in elems_price] + price  \n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get discount\n",
    "        elems_discount = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giamoi')\n",
    "        discount = [elem.text for elem in elems_discount] + discount\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get review counts:\n",
    "        elems_review_count = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_count_by')\n",
    "        review_count = [get_reviews_counts(elem.text) for elem  in elems_review_count] + review_count\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get quantity_sold\n",
    "        elems_quantity_sold = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_count_by')\n",
    "        quantity_sold = [elem.text.strip('\\n ') for elem  in elems_quantity_sold] + quantity_sold\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        # Get product_variant\n",
    "        elems_product_variant = driver.find_elements(By.CSS_SELECTOR, \".ProductGrid__grid .block_info_item_sp\")\n",
    "        product_variant = [elem.get_attribute('data-variant') for elem  in elems_product_variant] + product_variant\n",
    "        sleep(random.randint(5,7))\n",
    "\n",
    "        print(f\"Scraped IDs from page {page}\")\n",
    "        sleep(random.randint(8,15))\n",
    "    \n",
    "    return product_ids, data_product_ids, links, title, price, discount, review_count, quantity_sold, product_variant\n",
    "\n",
    "# Initialize browsser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL to crawl\n",
    "start_url = \"https://hasaki.vn/danh-muc/cham-soc-da-mat-c4.html?\"\n",
    "\n",
    "# Number pages to crawl\n",
    "num_pages = 2\n",
    "\n",
    "# Get data crawled\n",
    "product_ids, data_product_ids, links, title, price, discount, review_count, quantity_sold, product_variant = crawl_all_ids(driver, start_url, num_pages)\n",
    "\n",
    "# Create dataframe for data crawled\n",
    "data = pd.DataFrame(\n",
    "    list(zip(product_ids, data_product_ids, links, title,\n",
    "        price, discount, quantity_sold, product_variant, review_count\n",
    "    )),\n",
    "    columns=[\n",
    "        'product_id', 'data_product_id', 'link_item', 'title', \n",
    "        'original_price', 'current_price', 'quantity_sold', 'product_variant','review_count'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save into csv\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "productdata_filename = f\"productdata_{current_datetime}.csv\"\n",
    "folder_path = \"D:/HOC KI 8/3. Graduate project/hasaki_crawling\"\n",
    "\n",
    "data.to_csv(os.path.join(folder_path, \"data\", productdata_filename), encoding= \"utf-8-sig\")\n",
    "print(\"All product IDs saved to CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
