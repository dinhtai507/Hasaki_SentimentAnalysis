{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import random\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"D:/HOC KI 8/3. Graduate project/hasaki_crawling\"\n",
    "\n",
    "# Add user agen\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Setting options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Declare browser\n",
    "driver = webdriver.Chrome(options=options)\n",
    "sleep(random.randint(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ids from crawled csv\n",
    "productdata_filename = \"productdata_20240318_2350.csv\"\n",
    "\n",
    "existing_df = pd.read_csv(os.path.join(folder_path, \"data\", productdata_filename))\n",
    "existing_product_ids = existing_df['product_id'].tolist()\n",
    "existing_data_product_ids = existing_df['data_product_id'].tolist()\n",
    "existing_links = existing_df['link_item'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl Page 1\n",
      "['Thu Hương', 'Lưu văn Quân', 'lê thị như quỳnh', 'Vy Thao', 'Nguyễn Hoa', 'Mỹ Mỹ Trịnh', 'Trần Thị Thu Thuỷ', 'Vu thi Nga', 'Uyen Le', 'Uyen Le']\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "['Ngọc Lý', 'Ngô thị ngư ý', 'Hoài anh', 'trinh nguyễn', 'Phan Hải', 'VAnh', 'Tú Vân', 'Kim An', 'Linh Linh', 'Xuân Trang']\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "['TRINH NGUYỄN THỊ THÙY', 'nguyen hong ngoc thuy', 'NiNi', 'Nguyễn Tường An', 'Trần Thu Cần', 'Hồ Thị Hiền', 'Nguyễn thị thủy', 'Nguyễn Thị Thanh Thi', 'Mỹ Kim', 'Mani Ho']\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "['Sô Thu', 'La Minh Dũng', 'KIM YẾN', 'Thi', 'minh ngọc', 'CHỊ THẢO', 'Hạ uyên', 'Yến vy', 'Nguyễn Ngọc Cát Tường', 'Uyen Le']\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "['Phùng Tạ Linh', 'Ngoc Anh', 'Hoaithu25071993', 'Nguyễn Cát Tường', 'Hảo Phan', 'Vũ thu duyên', 'Nguyễn Thị Cẩm Dung', 'Lê Thuý Hoài', 'Minh Trang', 'Mai Đặng']\n",
      "Clicked on button next page!\n"
     ]
    }
   ],
   "source": [
    "# Get rating for comments:\n",
    "def get_star(string):\n",
    "    start_index = string.find(':')\n",
    "    end_index = string.find('%')\n",
    "    return int(string[start_index+1:end_index]) / 20\n",
    "\n",
    "# Process data-product-id\n",
    "def get_unique_data_productids(nested_list):\n",
    "    unique_ids = set()\n",
    "    for sublist in nested_list:\n",
    "        unique_ids.update(sublist.split(','))\n",
    "    return [id for id in unique_ids]\n",
    "\n",
    "# Parse data_product_id\n",
    "def parse_data_product_id(data_product_id_str):\n",
    "    # Split string by \",\"\n",
    "    id_list = data_product_id_str.split(',')\n",
    "    # Get unique\n",
    "    set_list = set()\n",
    "    set_list.update(id_list)\n",
    "    # Convert each element in the list to an integer and return\n",
    "    return [id_ for id_ in set_list]\n",
    "\n",
    "# ============================GET INFOMATION OF ALL ITEMS\n",
    "crawled_ids = set()\n",
    "df_list = []\n",
    "for i, row in existing_df[:6].iterrows():\n",
    "    \n",
    "    # Get product page\n",
    "    name_comment, content_comment, product_variant, datetime_comment, rating_comment = [], [], [], [], []\n",
    "    driver.get(row['link_item'])\n",
    "    \n",
    "    # Get data_product_id_list\n",
    "    elems_data_productids_list = driver.find_elements(By.CSS_SELECTOR, '.attribute-option-item')\n",
    "    uniq_data_productids_list = parse_data_product_id(\",\".join([elem.get_attribute('data-product-ids') for elem in elems_data_productids_list]))\n",
    "    uniq_data_product_id_str = \",\".join(uniq_data_productids_list)\n",
    "\n",
    "    # Get comment_pagination_number\n",
    "    elems_cmtpage_nums = driver.find_elements(By.CSS_SELECTOR, '.pagination_comment a')\n",
    "    commentpage_nums = [int(elem.get_attribute('rel')) for elem in elems_cmtpage_nums\n",
    "                    if elem.get_attribute('rel').isdigit()]\n",
    "    max_cmtpage = (max(commentpage_nums) if commentpage_nums else 1)\n",
    "    # Decide whether to crawl\n",
    "    if not set(uniq_data_productids_list).intersection(crawled_ids):\n",
    "        # Get comment details\n",
    "        for page_num in range(1, 2):\n",
    "            try:\n",
    "                print(\"Crawl Page \" + str(page_num))\n",
    "                elems_name = driver.find_elements(By.CSS_SELECTOR , \".title_comment strong.txt_color_1\")\n",
    "                name_comment = [elem.text for elem in elems_name] + name_comment\n",
    "                print(name_comment)\n",
    "\n",
    "                elems_content = driver.find_elements(By.CSS_SELECTOR , \".item_comment .content_comment\")\n",
    "                content_comment = [elem.text for elem in elems_content] + content_comment\n",
    "                \n",
    "                elems_product_variant = driver.find_elements(By.CSS_SELECTOR , \".item_comment .txt_999\")\n",
    "                product_variant = [elem.text for elem in elems_product_variant] + product_variant\n",
    "                \n",
    "                elems_datetime = driver.find_elements(By.CSS_SELECTOR , \".item_comment .timer_comment\")\n",
    "                datetime_comment = [elem.text for elem in elems_datetime] + datetime_comment\n",
    "\n",
    "                elems_rating = driver.find_elements(By.CSS_SELECTOR , \".item_comment .number_start\")\n",
    "                rating_comment = [get_star(elem.get_attribute('style')) for elem in elems_rating] + rating_comment\n",
    "\n",
    "                next_pagination_cmt = driver.find_element(By.CSS_SELECTOR, \"a.item_next_sort .icon_carret_down\")\n",
    "                next_pagination_cmt.click()\n",
    "\n",
    "                print(\"Clicked on button next page!\")\n",
    "                sleep(random.randint(5,7))\n",
    "\n",
    "            except ElementNotInteractableException:\n",
    "                print(\"Element Not Interactable Exception!\")\n",
    "                break\n",
    "\n",
    "        # Add into a dataframe\n",
    "        comment_data = pd.DataFrame(\n",
    "            list(zip(name_comment, content_comment, product_variant, datetime_comment, rating_comment)), \n",
    "            columns = ['name_comment', 'content_comment','product_variant', 'datetime_comment', 'rating'])\n",
    "        \n",
    "        # Add column \"link_item\", \"data_product_id_list\", \"data_product_id\"\n",
    "        comment_data.insert(0, \"link_item\", row['link_item'])\n",
    "        comment_data.insert(1, \"data_product_id_list\", uniq_data_product_id_str)\n",
    "        comment_data.insert(2, \"data_product_id\", row['data_product_id'])\n",
    "        \n",
    "        # For \"data_product_id_list\", convert string into list\n",
    "        comment_data['data_product_id_list'] = comment_data['data_product_id_list'].apply(parse_data_product_id)\n",
    "        df_list.append(comment_data)\n",
    "\n",
    "        crawled_ids.update(uniq_data_productids_list)\n",
    "        sleep(random.randint(7,9))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_item</th>\n",
       "      <th>data_product_id_list</th>\n",
       "      <th>data_product_id</th>\n",
       "      <th>name_comment</th>\n",
       "      <th>content_comment</th>\n",
       "      <th>product_variant</th>\n",
       "      <th>datetime_comment</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...</td>\n",
       "      <td>[99707, 102557, 5294, 19286, 19325, 95711, 529...</td>\n",
       "      <td>19325</td>\n",
       "      <td>Thu Hương</td>\n",
       "      <td>mình siêu ưng em này luôn, đợt đc tặng sn mà g...</td>\n",
       "      <td>Nước Tẩy Trang L'Oreal Dưỡng Ẩm Cho Da Thường,...</td>\n",
       "      <td>00: 15 | 08/03/2024</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...</td>\n",
       "      <td>[99707, 102557, 5294, 19286, 19325, 95711, 529...</td>\n",
       "      <td>19325</td>\n",
       "      <td>Lưu văn Quân</td>\n",
       "      <td>tuyệt lắm nha</td>\n",
       "      <td>Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...</td>\n",
       "      <td>16: 05 | 06/03/2024</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...</td>\n",
       "      <td>[99707, 102557, 5294, 19286, 19325, 95711, 529...</td>\n",
       "      <td>19325</td>\n",
       "      <td>lê thị như quỳnh</td>\n",
       "      <td>một từ thôi \"tuyệt\"ko thắc mắc gì hết</td>\n",
       "      <td>Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...</td>\n",
       "      <td>21: 44 | 05/03/2024</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...</td>\n",
       "      <td>[99707, 102557, 5294, 19286, 19325, 95711, 529...</td>\n",
       "      <td>19325</td>\n",
       "      <td>Vy Thao</td>\n",
       "      <td>Đây là nước tẩy trang mà tui cảm giác ổn nhất,...</td>\n",
       "      <td>Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...</td>\n",
       "      <td>11: 28 | 02/01/2024</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...</td>\n",
       "      <td>[99707, 102557, 5294, 19286, 19325, 95711, 529...</td>\n",
       "      <td>19325</td>\n",
       "      <td>Nguyễn Hoa</td>\n",
       "      <td>Thương hiệu xứng đáng điểm 10</td>\n",
       "      <td>Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...</td>\n",
       "      <td>11: 04 | 05/12/2023</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           link_item  \\\n",
       "0  https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...   \n",
       "1  https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...   \n",
       "2  https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...   \n",
       "3  https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...   \n",
       "4  https://hasaki.vn/san-pham/nuoc-tay-trang-tuoi...   \n",
       "\n",
       "                                data_product_id_list  data_product_id  \\\n",
       "0  [99707, 102557, 5294, 19286, 19325, 95711, 529...            19325   \n",
       "1  [99707, 102557, 5294, 19286, 19325, 95711, 529...            19325   \n",
       "2  [99707, 102557, 5294, 19286, 19325, 95711, 529...            19325   \n",
       "3  [99707, 102557, 5294, 19286, 19325, 95711, 529...            19325   \n",
       "4  [99707, 102557, 5294, 19286, 19325, 95711, 529...            19325   \n",
       "\n",
       "       name_comment                                    content_comment  \\\n",
       "0         Thu Hương  mình siêu ưng em này luôn, đợt đc tặng sn mà g...   \n",
       "1      Lưu văn Quân                                      tuyệt lắm nha   \n",
       "2  lê thị như quỳnh              một từ thôi \"tuyệt\"ko thắc mắc gì hết   \n",
       "3           Vy Thao  Đây là nước tẩy trang mà tui cảm giác ổn nhất,...   \n",
       "4        Nguyễn Hoa                      Thương hiệu xứng đáng điểm 10   \n",
       "\n",
       "                                     product_variant     datetime_comment  \\\n",
       "0  Nước Tẩy Trang L'Oreal Dưỡng Ẩm Cho Da Thường,...  00: 15 | 08/03/2024   \n",
       "1  Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...  16: 05 | 06/03/2024   \n",
       "2  Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...  21: 44 | 05/03/2024   \n",
       "3  Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...  11: 28 | 02/01/2024   \n",
       "4  Nước Tẩy Trang L'Oreal Tươi Mát Cho Da Dầu, Hỗ...  11: 04 | 05/12/2023   \n",
       "\n",
       "   rating  \n",
       "0     5.0  \n",
       "1     5.0  \n",
       "2     5.0  \n",
       "3     5.0  \n",
       "4     5.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all comment crawled\n",
    "combined_comment_data = pd.concat(df_list, ignore_index=True)\n",
    "combined_comment_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv\n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "comment_data_file_name = f\"comment_data_{current_datetime}.csv\"\n",
    "combined_comment_data.to_csv(os.path.join(folder_path, \"data\", comment_data_file_name), encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
