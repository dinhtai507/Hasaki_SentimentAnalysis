{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "\n",
    "# Add user agen\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Setting options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "options.add_argument(\"--no-sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ids from crawled csv\n",
    "productdata_filename = \"mergedproduct_20240326_1926.csv\"\n",
    "\n",
    "existing_df = pd.read_csv(os.path.join(folder_path, \"merged\", productdata_filename))\n",
    "existing_product_ids = existing_df['product_id'].tolist()\n",
    "existing_data_product_ids = existing_df['data_product_id'].tolist()\n",
    "existing_links = existing_df['link_item'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 2\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 2\n",
      "Clicked on button next page!\n",
      "Crawl Page 3\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 2\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 2\n",
      "Clicked on button next page!\n",
      "Crawl Page 3\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Clicked on button next page!\n",
      "Crawl Page 2\n",
      "Clicked on button next page!\n",
      "Crawl Page 3\n",
      "Clicked on button next page!\n",
      "Crawl Page 4\n",
      "Clicked on button next page!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n",
      "Crawl Page 1\n",
      "Next page button not found or not clickable!\n"
     ]
    }
   ],
   "source": [
    "# Get rating for comments:\n",
    "def get_star(string):\n",
    "    start_index = string.find(':')\n",
    "    end_index = string.find('%')\n",
    "    return int(string[start_index+1:end_index]) / 20\n",
    "\n",
    "# Process data-product-id\n",
    "def get_unique_data_productids(nested_list):\n",
    "    unique_ids = set()\n",
    "    for sublist in nested_list:\n",
    "        unique_ids.update(sublist.split(','))\n",
    "    return [id for id in unique_ids]\n",
    "\n",
    "# Parse data_product_id\n",
    "def parse_data_product_id(data_product_id_str):\n",
    "    # Split string by \",\"\n",
    "    id_list = data_product_id_str.split(',')\n",
    "    # Get unique\n",
    "    set_list = set()\n",
    "    set_list.update(id_list)\n",
    "    # Convert each element in the list to an integer and return\n",
    "    return [id_ for id_ in set_list]\n",
    "\n",
    "# ============================ GET INFOMATION OF ALL ITEMS\n",
    "# Declare browser\n",
    "driver = webdriver.Chrome(options=options)\n",
    "sleep(random.randint(1,5))\n",
    "\n",
    "crawled_ids = set()\n",
    "df_list = []\n",
    "# [1:40+1] ~ 1-40\n",
    "# [41:80+1] ~ 41-80\n",
    "# [81:120+1] ~ 81-120\n",
    "# [120:160+1] ~ 121-160 \n",
    "# [161:200+1] ~ 161-200\n",
    "# [201:240+1] ~ 201-240\n",
    "\n",
    "# [241:280+1] ~ 241-280\n",
    "# [281:320+1] ~ 281-320\n",
    "\n",
    "# [320:360+1] ~ 321-360 \n",
    "# [361:400+1] ~ 361-400\n",
    "\n",
    "# [401:500+1] ~ 401-500 \n",
    "# [501:600+1] ~ 501-600\n",
    "\n",
    "# [601:700+1] ~ 601-700\n",
    "# [701:800+1] ~ 701-800\n",
    "\n",
    "# [801:900+1] ~ 801-900\n",
    "# [901:1000+1] ~ 901-1000\n",
    "\n",
    "# [1001:1100+1] ~ 1001-1100\n",
    "# [1101:1200+1] ~ 1101-1200\n",
    "\n",
    "# [1201:1300+1] ~ 1201-1300\n",
    "# [1301:1400+1] ~ 1301-1400\n",
    "\n",
    "# [1401:1500+1] ~ 1401-1500\n",
    "# [1501:1600+1] ~ 1501-1600\n",
    "\n",
    "# [1601:1700+1] ~ 1601-1700\n",
    "# [1701:1800+1] ~ 1701-1800\n",
    "for i, row in existing_df[1701:1800+1].iterrows():\n",
    "    \n",
    "    # Get product page\n",
    "    name_comment, content_comment, product_variant, datetime_comment, rating_comment = [], [], [], [], []\n",
    "    driver.get(row['link_item'])\n",
    "    sleep(random.randint(6,7))\n",
    "    \n",
    "    # Get data_product_id_list\n",
    "    elems_data_productids_list = driver.find_elements(By.CSS_SELECTOR, '.attribute-option-item')\n",
    "    uniq_data_productids_list = parse_data_product_id(\",\".join([elem.get_attribute('data-product-ids') for elem in elems_data_productids_list]))\n",
    "    uniq_data_product_id_str = \",\".join(uniq_data_productids_list)\n",
    "\n",
    "    # Get comment_pagination_number\n",
    "    elems_cmtpage_nums = driver.find_elements(By.CSS_SELECTOR, '.pagination_comment a')\n",
    "    if elems_cmtpage_nums:\n",
    "        commentpage_nums = [int(elem.get_attribute('rel')) for elem in elems_cmtpage_nums\n",
    "                        if elem.get_attribute('rel').isdigit()]\n",
    "        max_cmtpage = max(commentpage_nums) if commentpage_nums else 1\n",
    "    else:\n",
    "        max_cmtpage = 1\n",
    "\n",
    "    # Decide whether to crawl\n",
    "    if not set(uniq_data_productids_list).intersection(crawled_ids):\n",
    "        # Get comment details\n",
    "        for page_num in range(1, max_cmtpage + 1):\n",
    "            try:\n",
    "                sleep(random.randint(2,3))\n",
    "                \n",
    "                print(\"Crawl Page \" + str(page_num))\n",
    "                elems_name = driver.find_elements(By.CSS_SELECTOR , \".title_comment strong.txt_color_1\")\n",
    "                name_comment = [elem.text for elem in elems_name] + name_comment\n",
    "                sleep(random.randint(1,2))\n",
    "\n",
    "                elems_content = driver.find_elements(By.CSS_SELECTOR , \".item_comment .content_comment\")\n",
    "                content_comment = [elem.text for elem in elems_content] + content_comment\n",
    "                sleep(random.randint(1,2))\n",
    "\n",
    "                elems_product_variant = driver.find_elements(By.CSS_SELECTOR , \".item_comment .txt_999\")\n",
    "                product_variant = [elem.text for elem in elems_product_variant] + product_variant\n",
    "                sleep(random.randint(1,2))\n",
    "\n",
    "                elems_datetime = driver.find_elements(By.CSS_SELECTOR , \".item_comment .timer_comment\")\n",
    "                datetime_comment = [elem.text for elem in elems_datetime] + datetime_comment\n",
    "                sleep(random.randint(1,2))\n",
    "\n",
    "                elems_rating = driver.find_elements(By.CSS_SELECTOR , \".item_comment .number_start\")\n",
    "                rating_comment = [get_star(elem.get_attribute('style')) for elem in elems_rating] + rating_comment\n",
    "                sleep(random.randint(1,2))\n",
    "                \n",
    "                next_pagination_cmt = driver.find_element(By.CSS_SELECTOR, \"a.item_next_sort .icon_carret_down\")\n",
    "                next_pagination_cmt.click()\n",
    "\n",
    "                print(\"Clicked on button next page!\")\n",
    "                sleep(random.randint(2,3))\n",
    "\n",
    "            except ElementNotInteractableException:\n",
    "                print(\"Element Not Interactable Exception!\")\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                print(\"Next page button not found or not clickable!\")\n",
    "                break        \n",
    "\n",
    "        # Add into a dataframe\n",
    "        comment_data = pd.DataFrame(\n",
    "            list(zip(name_comment, content_comment, product_variant, datetime_comment, rating_comment)), \n",
    "            columns = ['name_comment', 'content_comment','product_variant', 'datetime_comment', 'rating'])\n",
    "        \n",
    "        # Add column \"link_item\", \"data_product_id_list\", \"data_product_id\"\n",
    "        comment_data.insert(0, \"link_item\", row['link_item'])\n",
    "        comment_data.insert(1, \"data_product_id_list\", uniq_data_product_id_str)\n",
    "        comment_data.insert(2, \"data_product_id\", row['data_product_id'])\n",
    "        \n",
    "        # For \"data_product_id_list\", convert string into list\n",
    "        comment_data['data_product_id_list'] = comment_data['data_product_id_list'].apply(parse_data_product_id)\n",
    "        df_list.append(comment_data)\n",
    "\n",
    "        crawled_ids.update(uniq_data_productids_list)\n",
    "        sleep(random.randint(6,7))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_18044\\2165371809.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_comment_data = pd.concat(df_list, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_item</th>\n",
       "      <th>data_product_id_list</th>\n",
       "      <th>data_product_id</th>\n",
       "      <th>name_comment</th>\n",
       "      <th>content_comment</th>\n",
       "      <th>product_variant</th>\n",
       "      <th>datetime_comment</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hasaki.vn/san-pham/dau-goi-pantene-nga...</td>\n",
       "      <td>[83565, 83567, 83559, 90313, 83561, 90319, 835...</td>\n",
       "      <td>83555</td>\n",
       "      <td>HƯƠNG GIANG</td>\n",
       "      <td>Hasaki giao hàng rất chất lượng, kĩ càng. Sp d...</td>\n",
       "      <td>Dầu Gội Pantene Ngăn Rụng Tóc 900ml Đã mua hàn...</td>\n",
       "      <td>22: 15 | 20/11/2023</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hasaki.vn/san-pham/dau-goi-pantene-nga...</td>\n",
       "      <td>[83565, 83567, 83559, 90313, 83561, 90319, 835...</td>\n",
       "      <td>83555</td>\n",
       "      <td>Phương Lưu</td>\n",
       "      <td>ok lắm nha đúng là có làm chậm quá trình hư tổ...</td>\n",
       "      <td>Dầu Gội Pantene Phục Hồi Hư Tổn 650ml Đã mua h...</td>\n",
       "      <td>22: 24 | 26/02/2023</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hasaki.vn/san-pham/dau-goi-pantene-nga...</td>\n",
       "      <td>[83565, 83567, 83559, 90313, 83561, 90319, 835...</td>\n",
       "      <td>83555</td>\n",
       "      <td>Ngọc Uyên</td>\n",
       "      <td>Đúng với công dụng của sp, siêu mềm mượt óng ả...</td>\n",
       "      <td>Dầu Gội Pantene Dưỡng Tóc Suôn Mượt Óng Ả 900m...</td>\n",
       "      <td>23: 23 | 18/10/2022</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hasaki.vn/san-pham/dau-goi-pantene-nga...</td>\n",
       "      <td>[83565, 83567, 83559, 90313, 83561, 90319, 835...</td>\n",
       "      <td>83555</td>\n",
       "      <td>Nguyễn Hồng Ngân</td>\n",
       "      <td>Mình chưa dùng qua quá nhiều loại dầu gội nhưn...</td>\n",
       "      <td>Dầu Gội Pantene Hair Fall Control Ngăn Rụng Tó...</td>\n",
       "      <td>18: 34 | 26/04/2022</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hasaki.vn/san-pham/dau-goi-pantene-nga...</td>\n",
       "      <td>[83565, 83567, 83559, 90313, 83561, 90319, 835...</td>\n",
       "      <td>83555</td>\n",
       "      <td>Hoai Mac Pham</td>\n",
       "      <td>Hàng chính hãng, đóng gói cẩn thận, giá cạnh t...</td>\n",
       "      <td>Dầu Gội Pantene Làm Sạch Và Ngăn Ngừa Gàu 650m...</td>\n",
       "      <td>22: 05 | 04/05/2021</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           link_item  \\\n",
       "0  https://hasaki.vn/san-pham/dau-goi-pantene-nga...   \n",
       "1  https://hasaki.vn/san-pham/dau-goi-pantene-nga...   \n",
       "2  https://hasaki.vn/san-pham/dau-goi-pantene-nga...   \n",
       "3  https://hasaki.vn/san-pham/dau-goi-pantene-nga...   \n",
       "4  https://hasaki.vn/san-pham/dau-goi-pantene-nga...   \n",
       "\n",
       "                                data_product_id_list  data_product_id  \\\n",
       "0  [83565, 83567, 83559, 90313, 83561, 90319, 835...            83555   \n",
       "1  [83565, 83567, 83559, 90313, 83561, 90319, 835...            83555   \n",
       "2  [83565, 83567, 83559, 90313, 83561, 90319, 835...            83555   \n",
       "3  [83565, 83567, 83559, 90313, 83561, 90319, 835...            83555   \n",
       "4  [83565, 83567, 83559, 90313, 83561, 90319, 835...            83555   \n",
       "\n",
       "       name_comment                                    content_comment  \\\n",
       "0       HƯƠNG GIANG  Hasaki giao hàng rất chất lượng, kĩ càng. Sp d...   \n",
       "1        Phương Lưu  ok lắm nha đúng là có làm chậm quá trình hư tổ...   \n",
       "2         Ngọc Uyên  Đúng với công dụng của sp, siêu mềm mượt óng ả...   \n",
       "3  Nguyễn Hồng Ngân  Mình chưa dùng qua quá nhiều loại dầu gội nhưn...   \n",
       "4     Hoai Mac Pham  Hàng chính hãng, đóng gói cẩn thận, giá cạnh t...   \n",
       "\n",
       "                                     product_variant     datetime_comment  \\\n",
       "0  Dầu Gội Pantene Ngăn Rụng Tóc 900ml Đã mua hàn...  22: 15 | 20/11/2023   \n",
       "1  Dầu Gội Pantene Phục Hồi Hư Tổn 650ml Đã mua h...  22: 24 | 26/02/2023   \n",
       "2  Dầu Gội Pantene Dưỡng Tóc Suôn Mượt Óng Ả 900m...  23: 23 | 18/10/2022   \n",
       "3  Dầu Gội Pantene Hair Fall Control Ngăn Rụng Tó...  18: 34 | 26/04/2022   \n",
       "4  Dầu Gội Pantene Làm Sạch Và Ngăn Ngừa Gàu 650m...  22: 05 | 04/05/2021   \n",
       "\n",
       "   rating  \n",
       "0     5.0  \n",
       "1     5.0  \n",
       "2     5.0  \n",
       "3     5.0  \n",
       "4     5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all comment crawled\n",
    "combined_comment_data = pd.concat(df_list, ignore_index=True)\n",
    "combined_comment_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "comment_data_file_name = f\"comment_data_{current_datetime}.csv\"\n",
    "combined_comment_data.to_csv(os.path.join(folder_path, \"comment\", comment_data_file_name), encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
